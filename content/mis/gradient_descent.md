Title: Gradient Descent
Date: 2015-12-25 15:54
Slug: gradient-descent
Authors: Liwen Wen

Problem Setup: Have some function(loss functions): $J(\theta_0, \theta_1)$ and want to minimize it

So we start off with some random values of $\theta_0$ and $\theta_1$.

Repeately do $\theta_j = \theta_j - \alpha\frac{d J(\theta_0, \theta_1)}{d\theta_j}$ we have to do this for different variables simultaneously

What is stochastic gradient descent?
